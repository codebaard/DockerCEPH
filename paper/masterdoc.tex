\documentclass[titlepage, a4paper, 11pt]{scrartcl}

%too much whitespace otherwise
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}

% Grafik Pakete
\usepackage{graphicx,hyperref,amssymb}

% Ordner f√ºr Grafiken
\graphicspath{ {./images/} }
\usepackage{float}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}

\usepackage{caption}
\usepackage{subcaption}

% Header and Footer
\usepackage{fancyhdr}

%bibtex
\usepackage{cite}
\usepackage{csquotes}

%code snippets
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
    language=HTML,
    aboveskip=3mm,
    belowskip=3mm,
    showstringspaces=false,
    columns=flexible,
    basicstyle={\small\ttfamily},
    numbers=none,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue},
    commentstyle=\color{dkgreen},
    stringstyle=\color{mauve},
    breaklines=true,
    breakatwhitespace=true,
    tabsize=3
}

%dynamic image scaling
\newcommand{\imgScale}{0.7}

\pagestyle{fancy}
\fancyhf{}
\rhead{Julius Neudecker, 2025850, Page  \thepage}
\lhead{CEPH Cluster in containers}

%page numbering
\pagenumbering{arabic}

\title{Running a CEPH-Cluster from a containerized infrastructure}
\subtitle{Performance test with a mySQL-Database}
\author{Julius Neudecker \\ Bachelor of Science \\ \href{mailto:julius.neudecker@haw-hamburg.de}{julius.neudecker@haw-hamburg.de}}
\date{April 2021}


\begin{document}

    \maketitle

    \tableofcontents

    \begin{abstract}
        %Setting up and operating a storage cluster with high availability is a complex task. 
        %Modern paradigms like containerization and orchestration are a way of abstracting away some these complexities.
        %However, running a fault resistant storage cluster in a stateless and ephemeral containerized environment might seem contradicting at first.
        %Therefore the following paper will adress these problems, scrutinize performance and point out the inherent keypoints of this concept.
        %To put this analysis into context, a mySQL database will act as a use case to create a frame of reference.

        %An abstract summarizes, usually in one paragraph of 300 words or less, the major aspects of the entire paper in a prescribed sequence that includes: 
        %1) the overall purpose of the study and the research problem(s) you investigated; 
        %2) the basic design of the study; 
        %3) major findings or trends found as a result of your analysis; and, 
        %4) a brief summary of your interpretations and conclusions.

        Setting up and operating a storage cluster with high availability is a complex task. 
        By using containerization, it is possible to abstract away and encapsulate some repetitive tasks.
        Therefore this study aims to analyse the possibility, implications and findings of a multi host
        CEPH-Cluster, where the individual daemons are entirely running within a containerized environment.
        To put the findings into a frame of reference, this study utilizes a mySQL-database which has 
        special requirements on data storage. The key points in terms of advantages and disadvantages,
        data integrity, performance and administion are scrutinized. Major findings are that ...
        [insert findings here later] ... .
        Therefore running a distributed CEPH-storage in a containerization environment is ... 
        [draw conclusion] ... .



    \end{abstract}

    \section{Introduction}

        In times where information is a valuable asset, it is of paramount importance to have a scalable and reliable
        way of storing data and information. Considerations about data throughput and 
        IOPS\footnote{Input/Output Operations per Second} are also a major design parameter on modern storage solutions.
        These different storage solutions provide different approaches on these considerations. For any given use case, there
        exist several options to adress these. Depending on the architecture and scope of the problem some are better
        suited than others. A few major considerations are apart from scalability, reliability and speed also cost effectiveness,
        vendor lock-in, complexity and granular customizability.

        In general hardware based solutions have advantages in terms of raw performance but they often have significant disadvantages
        when it comes to vendor lock-in, easy scalability or restoration of corrupted disks. Software and network based solutions are
        \textit{in principle} less performant. However this can be mitigated for the most part by scaling up.

        Apart from propretary cloud storage providers i.e. AWS, Azure or Google, the \textbf{free market [correct term?]}
        is heavily dominated by CEPH by more than twice as much as the next competitor:

        \begin{figure}[H]
            \centering
            \fbox{\includegraphics[width=\imgScale\textwidth]{marketshareCeph.png}}
            \caption{Adoption of CEPH in OpenStack in 2016, \cite{cephadoption}}
            \label{fig:ceph-market-share}
        \end{figure}

        Being conceived by Sage Weil for his doctoral thesis\cite{weil2007ceph}, CEPH became part of the Linux Kernel in 2010 and was acquired
        by RedHat in 2014, CEPH is gaining popularity steadily since its introduction. \textbf{Source?}

        Another modern important concept which is increasingly shaping modern technology stacks is \textit{OS Level Virtualization} or
        \textit{containerization} as its colloquially called. This way of deploying applications decreased the complexity, which is
        inherent to deploying several different applications to one single host machine. \textbf{[/Refactor this section /Since when?]}

        Lastly, no storage solution exists without its use case. One major application, which requires flexible scaling are databases e.g. 
        relational databases. To guarantee deterministic behaviour, database servers need to have specific properties to be suitable
        as database providers. The underlying storage solution is one of these.

        \subsection{Problem domains}\label{introduction}
            
            Managing a highly available storage cluster is not a trivial task. Apart from provisioning and monitoring the hardware,
            setting up multiple systems concurrently is a daunting task. Nowadays with infrastructure automation tools like Salt, Chef or Puppet, 
            this is easier than ever. However, it can still be a tedious task to tweak the configuration of these tools
            in order to make it work on a complex or diverse infrastructure. Especially in times when upates and EOL\footnote{End Of Life} events
            create incompatibilites between working application stacks on any given host machine.

            As for CEPH, this is especially true, since there are three major versions in production\cite{cephreleases}, as of early 2021.
            Deprecated features and bugfixes create functional inconsistencies between major versions, hence it is a necessity to 
            keep a cluster in production up to date. This necessitates constant modification and testing of the previously mentioned automation tools.

            One way to isolate these problems is \textit{OS Level Virtualization}. By doing so, every application or application stack 
            exists within a so called \textit{Container} and is therefore isolated from the host to a certain degree. 
            One inherent issue in this context is that they are stateless and ephemeral. This means that they can 
            \textit{by principle} be created and deleted according to momentary requirements. This process can be fully automated
            by means of using an \textit{orchestration software}. Therefore the virtualized production environment has to be
            configured in a way that allows is to provide the stability which is required for a storage engine.

            Trying to overcome the difficulties in setting up and manage a CEPH cluster with containerization might seem contradicting
            at first sight. The following sections focus on the major problem and adresses these. There are many related topics such as
            further optimizations for one or another particular use case. To adress these would go beyond the scope of this paper.
            Nevertheless a brief outlook on further considerations will be provided at the end of chapter \ref*{analysis:tuning}.

        \subsection{Definition of research goal}   
        
            The goal of this research is to evaluate if setting up a CEPH-storage cluster with containers is possible and if so, 
            if it is a feasable option for a production environment. In order to make a conclusive assessment, three main points 
            have to be examined. In order to draw a meaningful conclusion, these three main topics must be evaluated in contrast
            to a \textit{non}-containerized cluster.

            \paragraph{Data Integrity}

                This means running a service on the cluster, which is very sensitive to data inconsistencies.
                In this particular example a mySQL database is chosen. As for reasons which will be discussed in 
                chapter \ref*{database:cluster}, Databases have some unique properties, which makes them more sensitive 
                to issues with data replication and keeping clustered storages in sync. 
                Therefore this use case is chosen as a suitable real world application.

            \paragraph{Performance}

                Since performance is a main consideration in production environments, this is next to data integrity
                the second most important concern. Depending on the overall cost structure of the environment, 
                a performance penalty might outweigh the benefits. In this case a containerized cluster 
                would be \textit{technically} possible but not economically feasable. Depending on the use case
                the important metric also differs. Fileserving services like storage clouds or streaming services 
                rely more on raw throughput. The data traffic with databases is generally rather small, therefore
                IOPS\footnote{Input/Output Operations per Seconds} are more important.

            \paragraph{Administration}

                One important reason to do reasearch in this topic is to evaluate if the time and effort to
                set up a cluster brings benefits in terms of administrative expenditure. At first sight a viable
                metric could be the spent time from starting to have a cluster up and running. However, depending
                on the production environment the results may vary to a wide degree. Therefore chapter \ref*{analysis:administration}
                will try to generate more abstract metrics for an objective evaluation.


        \subsection{Related Work}
        
            Since containerization and CEPH are already well established technologies, this section gives a brief overview of 
            other research which was already conducted in this area.
            RedHat in cooperation with Percona and Supermicro already conducted extensive research wheter it is feasable to
            run a SQL-Database on a CEPH-Cluster\cite{redhatstudy}. They concluded that indeed running a database on an 
            optimized CEPH-cluster exceeds industry standard database solutions. Furthermore scaling horizontally 
            is easier with a CEPH Cluster. Nevertheless it should be mentioned, that in this study the Percona 
            SQL distribution was utilized, which provides a native interface for kernel based RBD\footnote{Replicated Block Devices}.
            
            Hong et.al. used CEPH to create a framework which aims to mitigate handling issues with database containers.
            This includes the issues mentioned in chapter\ref*{introduction} that containers aren't persistent ways
            to store data\cite{hong2019database}. However this paper does not use a CEPH-cluster to host a database and
            has therefore only very little relevance to the topic of this paper. 
            
            Although there are numerous other papers
            which discuss the process of setting up CEPH itself or in the context of an OpenStack environment,
            none of them discussed a similar scope.            

    \section{Setting up CEPH on Docker}

        This section is about setting up CEPH with containerization. Firstly, an extensive insight into CEPH and its underlying
        structure is provided. Secondly The system-architecture for the experiments in the scope of this paper is explained.
        This second section will make a small detour into containerization, what it means, what the benefits and the drawbacks are.

        \subsubsection{Containerization}\label{system:containerization}

            %whats a container and why is it nice compared to VM, etc.
            Although in the previous text the term containerization was mentioned a few times already, there was no
            definitive definition of what a container or more general containerization. Firstly it is important to establish
            that containers are not \textit{something like} a virtual machine. In fact virtual machines are entire systems 
            which are executed on the hosts hypervisor\footnote{An abstraction layer for hardware and operating systems} and entirely self contained.
            This approach is really useful to deploy and maintain infrastructure since a machine with a defined behaviour can be
            launched in little time with no effort. However in terms of application development or microservices especially in the realm
            of CI/CD\footnote{Continuous Integration/Continuous Deployment} this is an uneccessary overhead.
            Containers on the other hand are in comparison small entities, which contain only the neccessary components
            of an application or service to run.

            \begin{figure}[H]
                \centering
                \fbox{\includegraphics[width=\imgScale\textwidth]{docker-vs-vm.png}}
                \caption{Containerization compared to Virtualization, \cite{2021_docker}}
                \label{fig:docker-vs-vm}
            \end{figure}     

            By deploying applications or services this way, problems such as confliciting software versions or 
            applications can be encapsulated, hence solved. This makes scaling also easier, since scaling doesn't
            have to happen on a virtual machine level, which might cause additional costs for licensing, 
            services can be scaled on a more granular basis with containers and orchestration tools like
            \textit{Docker Swarm} or \textit{Kubernetes}, which will be explained in detail in section \ref{system:containerization}.
            As of early 2021, there exists a vast variety of different containerization solutions such as 
            Rkt, LXD and Docker\cite{cncf}. However Docker is unarguably the biggest solution in terms of 
            adoption, available images\footnote{Docker images are the blueprints to start containers from} and community.

        \subsection{CEPH Architecture}\label{ceph:arch}

            In short CEPH is a distributed solution for storage clusters. On one side it is specifically tailored to provide
            maximum reliability by distributing data over different disks, machines or even datacenters and therefore also 
            improving overall scalability and performance. On the other side it is also a cost effective because it is open source\textbf{[Github Link]}
            and runs on commodity hardware. 

            The general structure includes several services which manage different functions within the cluster. In the context of
            CEPH these are called deamons, which are discussed in detail in the following sections.

            \begin{figure}[H]
                \centering
                \fbox{\includegraphics[width=\imgScale\textwidth]{ceph-architecture.jpg}}
                \caption{CEPH Architecture, \cite{hadlich_2015}}
                \label{fig:ceph-arch}
            \end{figure}
            
            Because it is a distributed system and may span over several physical machines, racks or even datacenters, inter-daemon
            communication plays a key role. For the purpose of autonomous data replication the cluster relies on a dedicated network.
            This network is separated from the public network, which is for client data access. Figure \ref{fig:ceph-arch} provides a 
            general overview.

            How the data is distributed internally is determined by two main factors: \textit{Placement Groups} and the \textit{CRUSH} algorithm.
            Placement Groups (abbreviated \textit{PGs}) are a way to keep track of the physical location of objects.
            Files are split up into different objects, where each object is stored in a different PG. Which one is determined by the 
            CRUSH\footnote{Controlled, Scalable, Decentralized Placement of Replicated Data} algorithm\cite{weil2007ceph}. Figure \ref{fig:ceph-crush}
            shows a schema of how this works.
            The reasoning behind PGs is that keeping track of millions of objects via metadata is computationally expensive\cite{PGcephDocu}.
            Therefore Objects are hashed and assigned to PGs.
            A so called CRUSH-Map keeps track of the structure of the whole cluster. This will become important in section \ref{system:crush-fail}.

            \subsubsection{Cluster Access}

                To access the cluster ceph provides different interfaces depending on the use-case:

                \begin{itemize}
                    \item \textit{CephFS} - A POSIX\footnote{Portable Operating System Interface - Part of the Unix specification} conform filesystem
                    \item \textit{LIBRBD/KRBD} - A block device
                    \item \textit{RADOSGW} - An REST gateway for storage buckets
                    \item \textit{LIBRADOS} - The API to access the cluster directly from applications
                \end{itemize}

                All these interfaces are based on LIBRADOS, provides the interface to access RADOS. This is the underlying object store
                responsible for distributing the data over several phsyical disks. This is done by the \textit{OSD}-Daemon.

                \paragraph{CephFS} is the filesystem, which allows for CEPH to behave to the user or the operating system like any other
                filesystem would. However is has to be distinguished from from block device. 
                The key is that it is not a filesystem like EXT4 or NTFS. It is more a translation layer to the
                RADOS\footnote{Reliable Autonomous Distributed Object Storage} interface, and provides features like snapshots. A required metadata service called \textit{MDS} provides journaling functionality and handle
                multi user data access. Because the throughput of the whole cluster scales linearly with the size of the cluster,
                often several different MDSs are required to handle the load and distribute the file metadata mitigate
                single points of failure. Is is best practice store and server the MDS from fast solid state drives to migigate bottlenecks.

                \paragraph{LIBRBD/KRBD} is a virtual block device. The difference to CephFS is that is behaves like a physical disk.
                Therefore it can be formatted with any filesystem like EXT4 or NTFS. The distinction between \textit{LIB}RBD and \textit{K}RBD
                means userspace for LIBRBD and kernelspace for KRBD. To explain these differences between these two would go beyond the 
                scope of this paper and requires knowledge about kernels, operating systems and memory handling. 

                \paragraph{RADOSGW} provides as REST gateway access storage buckets. Unlike filesystems or block devices, these storage
                buckets don't have a hierarchical structure with directories. All objects are stored on the same level, therefore the 
                analogy to a bucket. This bucket is addressed by means of an API which provides also a metadata filter to select
                specific files directly. This way of storing data is also compatible to Amazons S3\footnote{Simple Storage Service} product.
                A widely adopted usecase is serving static files on the internet for media content, javascript or css files by content delivery networks.
                They can also be used to store virtual disk images to be used in conjunction with RBDs.

                \paragraph{LIBRADOS} provides a native API to access RADOS directly without any file system or block device translation layer in
                between. The higher development costs for a more complex implementation might be worth the gains in throughput and IOPS
                for critical applications.

            \subsubsection{Object Storage Devices - OSD}

                Within the CEPH domain, these are the disks, where the actual data objects are stored to. 
                One or more OSDs can be handled from one CEPH OSD Daemon, which provides the connecting layer to the whole cluster.
                Also one OSD can contain several Placement Groups. This depends on the configuration of the cluster. On which disk and in which PG
                and object is stored is determined by the CRUSH algorithm as shown in figure \ref{fig:ceph-crush}.
                Since data is replicated within the cluster and constantly updated, one OSD could fail and the data and PGs of this failed
                OSD will be written to another OSD. This constant data shuffling takes place transparent to the user and is the reason why CEPH
                needs a dedicated network as mentioned in section \ref{ceph:arch}. How the data is physically stored on the disk differs between
                the two different backends used in older or newer releases.
            
            \subsubsection{Monitor Nodes - MON}

                Monitor Nodes (abbreviated as \textit{MON}) have two main purposes. They maintain a copy of the cluster map to hand this out 
                to a client, which connects to the cluster. Because of a fault this map might not reflect the state of the cluster accurately.
                Therefore an odd number of MON nodes have to agree on the current state of the cluster and distribute the correct cluster map.
                According to the CEPH documentation this is negotiated via the \textit{PAXOS} algorithm\cite{MONcephDocu}.
                Although a cluster could technically work with just one MON node, it is highly recommended to have at least three MON nodes,
                preventing a single point of failure.

                \begin{figure}[H]
                    \centering
                    \fbox{\includegraphics[width=\imgScale\textwidth]{Ceph-crush-map.png}}
                    \caption{Placement Groups and CRUSH Algorithm, \cite{krenn2020}}
                    \label{fig:ceph-crush}
                \end{figure}                           

            \subsubsection{Metadata Server - MDS}

                The metadata server is not neccessary for the cluster to operate as RBD, storage bucket or via Librados API.
                However to provide a journaled filesystem via the CephFS, this service is needed to provide the filesystem metadata such
                as permissions and timestamps. As stated from RedHat, it is best practice to deploy a MDS with big and fast nVME drives\cite{redhatstudy}.
                These can be also used in conjunction with memcached, where the recent metadata are stored into the RAM of the MDS.
                Overprovisioning, automatic failover and scalability are the main considerations for MDS to have high availability and high performance.

            \subsubsection{Manager - MGR}   
            
                Since the 12.x release, CEPH needs a manager node to operate\cite{MGRcephDocu}. This manager does not provide any
                functionality to the cluster itself but acts as an interface to external monitoring tools and systems. Without it 
                the state of the cluster won't appear to be healthy nor be externally visibly updated\cite{MGRcephDocu}.                

        \subsection{System Architecture}\label{system:my-setup}

            In this section I'll go over the system architecture in my experiments since its understanding is vital to the understanding
            of the configuration of the containers later on.

            Firstly, because of the Covid-19 pandemic I couldn't use the lab in the university so I had to rely on my personal ressources 
            in order to do my experiments. Since these are somewhat limited I configured a few things different than it would be done in reality otherwise.

            \textit{I assume that the reader has a basic understanding of Docker and how commands are executed or how containers work in detail. Details which are immaterial 
            to this topic will not be explained in great depth in the scope of this work.}

            \subsubsection{Hardware Diagram}

                The cluster is distributed across three different machines for storage. Since CEPH isn't available as Docker Image for 32bit and 64bit ARM architectures, 
                the only available x86\_64 host acts as Docker host for all containers including the OSDs. 
                However to create constraints such as network latency and bandwith, the physical volumes were
                attached to the ARM-Machines and mounted as iSCSI devices on the Docker host. Because in this small setup the bottlenecks weren't available CPU performance
                and RAM the performance impact of running a few more containers is negligible.

                \begin{figure}[H]
                    \centering
                    \fbox{\includegraphics[width=\imgScale\textwidth]{placeholder.jpg}}
                    \caption{Hardware Diagram}
                    \label{fig:hardware-diagram}
                \end{figure}   

                Furthermore is the Docker host a server in production, therefore a certain amount of around 10\% were already allocated to other running processes.
                Since my tests were exclusively conducted in the local network, the bandwith of the internet connection wasn't a concern either.
                Usually a CEPH cluster uses two seperate networks: one for replicating data between the OSDs and one to access the cluster for data or configuration.
                In this case only one physical network was available, which has a bandwith of 1GBit/s

            \subsubsection{Docker Config for CEPH Image}\label{config:docker}

                By starting CEPH as a container, a lot of configuration work is abstracted away in the stock Docker Image.
                The official image \textbf{INSERT LINK HERE} provides two options to distribute the configuration between the different nodes:

                \begin{itemize}
                    \item On disk
                    \item KV-Store\footnote{As of 04.2021 only etcd is supported by the official image}
                \end{itemize}

                The same Image provides all the different daemons, which are needed in a CEPH cluster and thus every container is configured different.
                The configuration is stored \textit{on Disk} because all containers are executed on the same physical host, hence a remote KV-Store isn't required.
                This is reflected by the two directories \textit{/docker/ceph/data/etc/} and \textit{/docker/ceph/data/lib}, which all containers have in common.
                
                Another issue is that the MON and OSD nodes run into timeout issues, when deployed on the same host and configured to use Dockers \textit{--net=host} option, as suggested
                by the official manual \textbf{insert link to docher hub here}.
                This would effectively bind the ports of any container, which is configured in that way directly to the host interface. 
                I chose another approach: providing each container with a dedicated IP-Adress within the network. This is possible by using the \textit{MACVLAN} network
                driver which is provided by the docker engine. This allows every container which has this driver enabled to appear as a seperate host apart from the docker host.
                The IP is configured with the \textit{--ip=192.167.178.2XX} option.

                The complete configuration is made available under a repository \textbf{INSERT CONFIG AND REPO HERE}.

                \paragraph{Monitor Nodes}

                    The network configuration of the monitor nodes is of special interest here: The public network has to be configured in case there are two 
                    distinct networks as explained in \textbf{refer to section}. This is not neccessary for the other nodes since the configuration will be distributed
                    after the monitor nodes are online.

                    \begin{lstlisting}
                        docker run -d \textbackslash
                        -v /docker/ceph/data/etc:/etc/ceph \textbackslash
                        -v /docker/ceph/data/lib:/var/lib/ceph/ \textbackslash
                        -e MON_IP=192.168.178.220 \textbackslash
                        -e CEPH_PUBLIC_NETWORK=192.168.178.0/24 \textbackslash
                        --ip=192.168.178.220 \textbackslash
                        --network=macvlan0 \textbackslash
                        --name ceph_mon1 ceph/daemon mon                        
                    \end{lstlisting}

                \paragraph{Manager Nodes}

                    This containers runs in parallel to a monitor node and doesn't need any special configuration.

                    \begin{lstlisting}                    
                        docker run -d \textbackslash
                        -v /docker/ceph/data/etc:/etc/ceph \textbackslash
                        -v /docker/ceph/data/lib:/var/lib/ceph/ \textbackslash
                        --ip=192.168.178.223 \textbackslash
                        --network=macvlan0 \textbackslash
                        --name ceph_mgr ceph/daemon mgr
                    \end{lstlisting}

                \paragraph{OSD Nodes}

                    The configuration of the OSD nodes is compared to the other nodes more complicated. I chose to provide a physical disk
                    to the container. Therefore it had to be initialized as a logical volume before the OSD node is able to utilize it.
                    The official CEPH image provides a configuration option for this purpose. The following command creates a temporary container,
                    which is configured by the \textit{--rm} command. The \textit{--data /dev/sdX} command specifies the physical disk, which will be 
                    pre-treated. A prerequisite is that any disk has to be completely wiped of any partition table. This is done by executing \textit{sgdisk}
                    on every single in question.

                    \begin{lstlisting}
                        docker run --rm \textbackslash
                        --privileged=true \textbackslash
                        -v /docker/ceph/data/lib:/var/lib/ceph/ \textbackslash
                        -v /var/log/ceph:/var/log/ceph/ \textbackslash
                        -v /docker/ceph/data/etc:/etc/ceph \textbackslash
                        -v /dev/:/dev/ \textbackslash
                        -v /run/lock/lvm:/run/lock/lvm:z \textbackslash
                        -v /run/lvm/:/run/lvm/ \textbackslash
                        -v /var/run/udev/:/var/run/udev/:z \textbackslash
                        --ip=192.168.178.230 \textbackslash
                        --network=macvlan0 \textbackslash
                        --entrypoint ceph-volume ceph/daemon lvm prepare \textbackslash
                        --bluestore \textbackslash
                        --data /dev/sdX \textbackslash
                        --no-systemd                            
                    \end{lstlisting} 

                    When the disk is prepared, it appears as \textit{lvm-partition} with a ceph-prefix. The OSD node can then be started by executing the following command:

                    \begin{lstlisting}
                        docker run -d \textbackslash
                        --privileged=true \textbackslash
                        --pid=host \textbackslash
                        -v /docker/ceph/data/etc:/etc/ceph \textbackslash
                        -v /docker/ceph/data/lib:/var/lib/ceph/ \textbackslash
                        -v /dev/:/dev/ \textbackslash
                        -v /run/udev/:/run/udev/ \textbackslash
                        -e OSD_DEVICE=/dev/sde \textbackslash
                        -e OSD_TYPE=disk \textbackslash
                        -e OSD_BLUESTORE=1 \textbackslash
                        -e CEPH_DAEMON=OSD_CEPH_VOLUME_ACTIVATE \textbackslash
                        -e OSD_ID=0 \textbackslash
                        --ip=192.168.178.230 \textbackslash
                        --network=macvlan0 \textbackslash
                        --name ceph_osd_sde ceph/daemon osd
                    \end{lstlisting}

                    There are some important things to consider here: firstly the \textit{--pid=host} configuration is neccessary to prevent that the host system
                    runs out of open file handles. This is a bug, which has to be fixed\textbf{INSERT LINK TO GITHUB ISSUE HERE}. The \textit{--privileged=true} setting
                    is neccessary to give the required access to disks from within the container. However from a security standpoint this is not good practice and should never be
                    used in that way in a production environment! \textit{OSD\_BLUESTORE=1} configures the way CEPH stores the data in the background and \textit{OSD\_ID=X} defines
                    the ID under which the OSD will be reffered to by the cluster.

                \paragraph{Metadata Nodes}

                    Since one phyisical host can host many different metadata nodes to provide peak performance, these mds nodes have to be named uniquely. This is done 
                    by using the \textit{\$hostname-x} variable. Also the official image is recommending to set the \textit{CEPHFS\_CREATE=1} flag. On my setup however this resulted in an 
                    error and the filesystem was not going to be created. In my case I had to create the filesystem manually from within the monitor container by means of a \textit{docker exec}
                    command. Therefore this flag has to be set to "0" in this case.

                    \begin{lstlisting}
                        docker run -d \textbackslash
                        -v /docker/ceph/data/lib:/var/lib/ceph/ \textbackslash
                        -v /docker/ceph/data/etc:/etc/ceph \textbackslash
                        -e CEPHFS_CREATE=0 \textbackslash
                        -e MDS_NAME=mds-$(hostname)-a \textbackslash
                        --ip=192.168.178.245 \textbackslash
                        --network=macvlan0 \textbackslash
                        --name ceph_mds_a ceph/daemon mds
                    \end{lstlisting}   
                    
            \subsubsection{CRUSH Fail mode}\label{system:crush-fail}

                In order to define the behaviour of the cluster in case of failures, CEPH provides a setting
                which is called \textit{CRUSH Fail Mode}. This setting defines the biggest possible failure domains, 
                which should not affect data integrity\cite{crushFailure}. In consequence, data is replicated in a
                way which guarantees this. To pick a few examples: 

                \paragraph{OSD (or device)} means that a single drive could fail and no data will be lost.
                This would be the preferred setting when the cluster runs on a single machine. However, if the cluster
                consists of more than one machine and the whole machine fails because of a PSU\footnote{Power Supplying Unit} failure
                for example, data integrity cannot be guaranteed anymore.

                \paragraph{Chassis} is the failure mode for sophisticated rack mounted servers, where a multitude of servers
                comprise the whole cluster. This would cover the previously mentioned case, which might be as well interesting for maintenance,
                when individual machines have to be maintained or decommissioned.
                
                \paragraph{Rack} provides data integrity across multiple chassis or hosts on a datacenter level. If for example
                a critical component of the network infrastructure fails, this could provide some security. Although CEPH clusters 
                are in principle set up in a way which prevents SPOF\footnote{Single Point Of Failure}, this must not be true for the
                peripheral infrastructure. 

                \paragraph{Datacenter} usually means that a huge number of several racks, consisting of multiple chassis are grouped
                together as a common failure domain - in case of a power outage or natural disaster. If the company which owns the datacenter,
                operates on a global scale this failure mode is also a consideration for performance. Because of physical and network
                topology, the RTT\footnote{Round Trip Time} to a certain datacenter might be longer than desired. If all data is replicated
                on datacenter granularity, a global scale data integrity is guaranteed as well as the possibly shortest response time.

                \paragraph{In general} the failure mode should be as granular as possible but also as fine as financially feasable.
                In the context of the system architecture in the scope of this paper (see section \ref{system:my-setup}), the fail mode will be on an OSD level.
                Because my experimental setup consists of multiple disks and flash storages with different ages and levels of degradation.
                However, the appropriate setting would be \textit{host}-level data replication. \textbf{[Revisit this section after finished setup]}

            \subsubsection{Creating the cluster}

                Although there are different solutions to automate the deployment of a cluster such as \textit{CEPH-Ansible}\textbf{ADD LINK} or \textit{CEPH for Rook}\textbf{ADD REPO}
                my cluster is rolled out and configured with a simple shellscript - \textit{startup.sh} - which creates a fully functional cluster:

                \begin{lstlisting}
                    #!/bin/sh

                    ## start MON and MGR
                    docker run -d -v /docker/ceph/data/etc:/etc/ceph -v /docker/ceph/data/lib:/var/lib/ceph/ -e MON_IP=192.168.178.220 -e CEPH_PUBLIC_NETWORK=192.168.178.0/24 --ip=192.168.178.220 --network=macvlan0 --name ceph_mon1 ceph/daemon mon
                    docker run -d -v /docker/ceph/data/etc:/etc/ceph -v /docker/ceph/data/lib:/var/lib/ceph/ --ip=192.168.178.223 --network=macvlan0 --name ceph_mgr ceph/daemon mgr

                    docker exec -it ceph_mon1 ceph auth get client.bootstrap-osd -o /var/lib/ceph/bootstrap-osd/ceph.keyring

                    ## prepare and create OSDs
                    ./prepare.sh
                    ./create.sh

                    ## create fs
                    docker exec -it ceph_mon1 ceph osd pool create cephfs_data
                    docker exec -it ceph_mon1 ceph osd pool create cephfs_metadata
                    docker exec -it ceph_mon1 ceph fs new cephfs cephfs_metadata cephfs_data

                    ##create the additional MON and MDS after the cluster has been created
                    docker run -d -v /docker/ceph/data/etc:/etc/ceph -v /docker/ceph/data/lib:/var/lib/ceph/ -e MON_IP=192.168.178.221 -e CEPH_PUBLIC_NETWORK=192.168.178.0/24 --ip=192.168.178.221 --network=macvlan0 --name ceph_mon2 ceph/daemon mon
                    docker run -d -v /docker/ceph/data/lib:/var/lib/ceph/ -v /docker/ceph/data/etc:/etc/ceph -e CEPHFS_CREATE=0 -e MDS_NAME=mds-$(hostname)-a --ip=192.168.178.245 --network=macvlan0 --name ceph_mds_a ceph/daemon mds

                    docker run -d -v /docker/ceph/data/etc:/etc/ceph -v /docker/ceph/data/lib:/var/lib/ceph/ -e MON_IP=192.168.178.222 -e CEPH_PUBLIC_NETWORK=192.168.178.0/24 --ip=192.168.178.222 --network=macvlan0 --name ceph_mon3 ceph/daemon mon
                    docker run -d -v /docker/ceph/data/lib:/var/lib/ceph/ -v /docker/ceph/data/etc:/etc/ceph -e CEPHFS_CREATE=0 -e MDS_NAME=mds-$(hostname)-b --ip=192.168.178.246 --network=macvlan0 --name ceph_mds_b ceph/daemon mds

                    exit 0
                \end{lstlisting}

                The docker run commands are the same as above but executed in a specific order. One monitor node has to be launched first to create an initial cluster configuration.
                Shortly thereafter follows a manager node. The next line deploys the keyring-file to the other bootstrap directories. Since all containers are deployed on a single machine, every node can access this keyring in its respective bootstrap directory. The script \textit{prepare.sh} and \textit{create.sh} executes the commands mentioned under the OSD part of section \ref*{config:docker} for each disk.
                %The next strep is the creation of the filesystem. According to the docker documentation the FS will be created upon launch of the MDS node with the \textit{CEPHFS\_CREATE=1} parameter defined. However this didn't work properly in my case. The lines with \textit{docker exec ...} execute a command within the denominated container (in this case 
                %\textit{ceph_mon1}) from the host shell. These three commands create the data pools and the filesystem, which requires as metadata pool and a data pool\textbf{ADD CEPH DOC HERE}.
                The following lines are only the additional monitor and mds nodes being created. These are not strictly neccessary for the cluster to function but provide redundancy to secure the \textit{no single point of failure} design of CEPH.

    \section{System Analysis}

        

        To measure the performance of the cluster, I chose to use two tools: \textit{scbench} and \textit{mysqlslap}. Both tools give detailed insight into
        the critical performances figures for such a cluster. As for the raw read/write performance test I can only evaluate the performance of the cluster at hand for reasons mentioned 
        in \ref*{system:my-setup}. However since my server has different storage technologies, I can evaluate the database performance with the cluster, an HDD and SSD.

        \subsection{SCBENCH} 

            % scbench: https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/4/html/administration_guide/ceph-performance-benchmarking
        
            This tool measures the raw performance of a CEPH cluster by performing a write and subsequent read test sequential and random\textbf{REF TO REDHAT}.
            I performed three different tests with different loads on the cluster:

            \begin{itemize}
                \item 10 seconds with 16 threads
                \item 30 seconds with 32 threads
                \item 60 seconds with 64 threads
            \end{itemize}

            My goal is to load the cluster in a way that eventually all caches and short term performance enhancements become saturated to gauge the stable long term performance.
            What exactly scbench does under the hood, is described in great detail under \textbf{RESEARCH LINK}.

            This test yields three different performance parametres for read and write actions:

            \begin{itemize}
                \item Bandwidth
                \item IOPS
                \item Latency
            \end{itemize}

            For reading the test was executed for sequential and random access of the data. However the tests for sequential read access weren't sufficient enough to evaluate, therefore I will only show the results for random read, which is more important in the context of database operations anyway.

            \textit{All graphs were created from the raw data, which scbench provides. The code to reproduce these results is available under}\textit{[ADD REPOLINK]}

            \paragraph{Write performance}

                Before executing the read performance test, \textit{scbench} has to execute the write test in order to have readable data available.

                \begin{figure}[H]
                    \centering
                    \fbox{\includegraphics[width=\imgScale\textwidth]{Write_Bandwidth.png}}
                    \caption{Normal distribution of the writespeed for three different tests}
                    \label{fig:write-speed}
                \end{figure}   

                Figure\ref*{fig:write-speed} shows the saturation of the write speed with increasing load. On write operations with a short duration,
                the cost of the whole procedure of contacting the cluster to receive the correct endpoints to write to is relatively high, therefore the initial
                period where no data is written has an overall bigger impact on the measurement. As the client is writing more data to the OSDs, the writespeed increases.
                However after the different caching entities within the cluster become saturated, the writespeed eventually stabilizes at around 2.5MB/s.

                \begin{figure}[H]
                    \centering
                    \fbox{\includegraphics[width=\imgScale\textwidth]{Write_IOPS.png}}
                    \caption{Normal distribution of the write IOPS for three different tests}
                    \label{fig:write-iops}
                \end{figure}   

                The write IOPS in figure \ref*{fig:write-iops} exhibits the same result: Initial IOPS start slow due to the cost of initializing, subsequently increasing as the initialization has a smaller impact on the results but with saturation decreasing again. 

                \begin{figure}[H]
                    \centering
                    \fbox{\includegraphics[width=\imgScale\textwidth]{Write_Latency.png}}
                    \caption{Normal distribution of the write latency for three different tests}
                    \label{fig:write-latency}
                \end{figure}   

                The write latency for the 10s and 30s mark shown in figure \ref*{fig:write-latency} is about the same. The saturated case after 60s however is an interesting anomaly. The best explanation is that the process of reorganizing data and placement groups takes away performance and therefore increasing latency.
                This is especially plausible since all data transactions are processed over the same physical network interface since this cluster does not have a dedicated network for this task\textbf{[INSERT NOTE TO SETUP HERE]}.

            \paragraph{Read performance}

                This test reads the previously created data during the write test randomly.

                \begin{figure}[H]
                    \centering
                    \fbox{\includegraphics[width=\imgScale\textwidth]{Read_Bandwidth_Rand.png}}
                    \caption{Normal distribution of the readspeed for three different tests}
                    \label{fig:read-speed}
                \end{figure}   

                The read spead starts at around 1700MB/s, which is multiple times more than the other more saturated tests were able to achieve. 
                Considered the hardware architecture of the cluster, this value is only really achievable with caching data within the RAM.
                In fact the hardware requirements for a CEPH-Cluster demand around 2GB of RAM per OSD\textbf{REFERENCE HERE}.

                \begin{figure}[H]
                    \centering
                    \fbox{\includegraphics[width=\imgScale\textwidth]{Read_IOPS_Rand.png}}
                    \caption{Normal distribution of the read IOPS for three different tests}
                    \label{fig:read-iops}
                \end{figure}   

                The IOPS paint the same picture: very high cluster speed on the first few seconds with a more stabilized value after saturation of the caching chain. An interesting point is the high standard deviation before everything becomes saturated. I presume that it is the timespan, where
                some disks cache is already saturated and other caches are still filling up. Also the CPU of the host machine has only four logical cores and therefore quickly becomes task-saturated. The other cores in the cluster aren't that fast either. This results in the value stabilizing after around a minute.

                \begin{figure}[H]
                    \centering
                    \fbox{\includegraphics[width=\imgScale\textwidth]{Read_Latency_Random.png}}
                    \caption{Normal distribution of the read latency for three different tests}
                    \label{fig:read-latency}
                \end{figure}  

                The very fast response in the first ten seconds is the best argument that the data actually is read from the RAM instead 
                of the disks. The very small standard deviation can't really be achieved with different kinds of disks on iSCSI hosts. Referring to figure \ref*{fig:write-latency}, it is likely the same mechanic of reorganizing placement groups and data objects within the cluster which leads to increased latency. Also in conjunction with task saturation of the host machine.

        \section{Mysqlslap}

            some lorem ipsum going on here...


    \section{Conclusion}

        \subsection{Advantages}

        %in comparison to conventional (Host Level)

        \subsection{Disadvantages}

        %in comparison to conventional (Host Level)        

        \subsection{Performance}
    
        % make remark about performance tuning and tiering -> beyond the scope of this
        % i.e. pool configs, stripe config, safety with crush config, etc.
        % also there might be another interface for mySQL needs like RADOSGW or RBD
        % but this would not be comparable to a single tennant mySQL application

        %which implementation of radoes does the container use? krbd vs librbd
        % remark to section with OSD in Architecture: "ease of use" vs "performance" tradeoff

        \subsection{In Summary}

        % summarize the keypoints in regard to the introduction
      

    \bibliography{references}        
    \bibliographystyle{IEEEtran}

\end{document}