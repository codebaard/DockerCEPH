\documentclass[titlepage, a4paper, 11pt]{scrartcl}

%too much whitespace otherwise
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}

% Grafik Pakete
\usepackage{graphicx,hyperref,amssymb}

% Ordner f√ºr Grafiken
\graphicspath{ {./images/} }
\usepackage{float}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}

\usepackage{caption}
\usepackage{subcaption}

% Header and Footer
\usepackage{fancyhdr}

%bibtex
\usepackage{cite}
\usepackage{csquotes}

%code snippets
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
    language=HTML,
    aboveskip=3mm,
    belowskip=3mm,
    showstringspaces=false,
    columns=flexible,
    basicstyle={\small\ttfamily},
    numbers=none,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue},
    commentstyle=\color{dkgreen},
    stringstyle=\color{mauve},
    breaklines=true,
    breakatwhitespace=true,
    tabsize=3
}

%dynamic image scaling
\newcommand{\imgScale}{0.7}

\pagestyle{fancy}
\fancyhf{}
\rhead{Julius Neudecker, 2025850, Page  \thepage}
\lhead{CEPH Cluster in containers}

%page numbering
\pagenumbering{arabic}

\title{Running a CEPH-Cluster from a containerized infrastructure}
\subtitle{Use case: mySQL-database}
\author{Julius Neudecker \\ Bachelor of Science \\ \href{mailto:julius.neudecker@haw-hamburg.de}{julius.neudecker@haw-hamburg.de}}
\date{January 2020}


\begin{document}

    \maketitle

    \tableofcontents

    \begin{abstract}
        %Setting up and operating a storage cluster with high availability is a complex task. 
        %Modern paradigms like containerization and orchestration are a way of abstracting away some these complexities.
        %However, running a fault resistant storage cluster in a stateless and ephemeral containerized environment might seem contradicting at first.
        %Therefore the following paper will adress these problems, scrutinize performance and point out the inherent keypoints of this concept.
        %To put this analysis into context, a mySQL database will act as a use case to create a frame of reference.

        %An abstract summarizes, usually in one paragraph of 300 words or less, the major aspects of the entire paper in a prescribed sequence that includes: 
        %1) the overall purpose of the study and the research problem(s) you investigated; 
        %2) the basic design of the study; 
        %3) major findings or trends found as a result of your analysis; and, 
        %4) a brief summary of your interpretations and conclusions.

        Setting up and operating a storage cluster with high availability is a complex task. 
        By using containerization, it is possible to abstract away and encapsulate some repetitive tasks.
        Therefore this study aims to analyse the possibility, implications and findings of a multi host
        CEPH-Cluster, where the individual daemons are entirely running within a containerized environment.
        To put the findings into a frame of reference, this study utilizes a mySQL-database which has 
        special requirements on data storage. The key points in terms of advantages and disadvantages,
        data integrity, performance and administion are scrutinized. Major findings are that ...
        [insert findings here later] ... .
        Therefore running a distributed CEPH-storage in a containerization environment is ... 
        [draw conclusion] ... .



    \end{abstract}

    \section{Introduction}

        In times where information is a valuable asset, it is of paramount importance to have a scalable and reliable
        way of storing data and information. Considerations about data throughput and 
        IOPS\footnote{Input/Output Operations per Second} are also a major design parameter on modern storage solutions.
        These different storage solutions provide different approaches on these considerations. For any given use case, there
        exist several options to adress these. Depending on the architecture and scope of the problem some are better
        suited than others. A few major considerations are apart from scalability, reliability and speed also cost effectiveness,
        vendor lock-in, complexity and granular customizability.

        In general hardware based solutions have advantages in terms of raw performance but they often have significant disadvantages
        when it comes to vendor lock-in, easy scalability or restoration of corrupted disks. Software and network based solutions are
        \textit{in principle} less performant. However this can be mitigated for the most part by scaling up.

        Apart from propretary cloud storage providers i.e. AWS, Azure or Google, the \textbf{free market [correct term?]}
        is heavily dominated by CEPH by more than twice as much as the next competitor:

        \begin{figure}[H]
            \centering
            \fbox{\includegraphics[width=\imgScale\textwidth]{marketshareCeph.png}}
            \caption{Adoption of CEPH in OpenStack in 2016, \cite{cephadoption}}
            \label{fig:ceph-market-share}
        \end{figure}

        Being conceived by Sage Weil for his doctoral thesis\cite{weil2007ceph}, CEPH became part of the Linux Kernel in 2010 and was acquired
        by RedHat in 2014, CEPH is gaining popularity steadily since its introduction. \textbf{Source?}

        Another modern important concept which is increasingly shaping modern technology stacks is \textit{OS Level Virtualization} or
        \textit{containerization} as its colloquially called. This way of deploying applications decreased the complexity, which is
        inherent to deploying several different applications to one single host machine. \textbf{[/Refactor this section /Since when?]}

        Lastly, no storage solution exists without its use case. One major application, which requires flexible scaling are databases e.g. 
        relational databases. To guarantee deterministic behaviour, database servers need to have specific properties to be suitable
        as database providers. The underlying storage solution is one of these.

        \subsection{Problem domains}\label{introduction}
            
            Managing a highly available storage cluster is not a trivial task. Apart from provisioning and monitoring the hardware,
            setting up multiple systems concurrently is a daunting task. Nowadays with infrastructure automation tools like Salt, Chef or Puppet, 
            this is easier than ever. However, it can still be a tedious task to tweak the configuration of these tools
            in order to make it work on a complex or diverse infrastructure. Especially in times when upates and EOL\footnote{End Of Life} events
            create incompatibilites between working application stacks on any given host machine.

            As for CEPH, this is especially true, since there are three major versions in production\cite{cephreleases}, as of early 2021.
            Deprecated features and bugfixes create functional inconsistencies between major versions, hence it is a necessity to 
            keep a cluster in production up to date. This necessitates constant modification and testing of the previously mentioned automation tools.

            One way to isolate these problems is \textit{OS Level Virtualization}. By doing so, every application or application stack 
            exists within a so called \textit{Container} and is therefore isolated from the host to a certain degree. 
            One inherent issue in this context is that they are stateless and ephemeral. This means that they can 
            \textit{by principle} be created and deleted according to momentary requirements. This process can be fully automated
            by means of using an \textit{orchestration software}. Therefore the virtualized production environment has to be
            configured in a way that allows is to provide the stability which is required for a storage engine.

            Trying to overcome the difficulties in setting up and manage a CEPH cluster with containerization might seem contradicting
            at first sight. The following sections focus on the major problem and adresses these. There are many related topics such as
            further optimizations for one or another particular use case. To adress these would go beyond the scope of this paper.
            Nevertheless a brief outlook on further considerations will be provided at the end of chapter \ref*{analysis:tuning}.

        \subsection{Definition of research goal}   
        
            The goal of this research is to evaluate if setting up a CEPH-storage cluster with containers is possible and if so, 
            if it is a feasable option for a production environment. In order to make a conclusive assessment, three main points 
            have to be examined. In order to draw a meaningful conclusion, these three main topics must be evaluated in contrast
            to a \textit{non}-containerized cluster.

            \paragraph{Data Integrity}

                This means running a service on the cluster, which is very sensitive to data inconsistencies.
                In this particular example a mySQL database is chosen. As for reasons which will be discussed in 
                chapter \ref*{database:cluster}, Databases have some unique properties, which makes them more sensitive 
                to issues with data replication and keeping clustered storages in sync. 
                Therefore this use case is chosen as a suitable real world application.

            \paragraph{Performance}

                Since performance is a main consideration in production environments, this is next to data integrity
                the second most important concern. Depending on the overall cost structure of the environment, 
                a performance penalty might outweigh the benefits. In this case a containerized cluster 
                would be \textit{technically} possible but not economically feasable. Depending on the use case
                the important metric also differs. Fileserving services like storage clouds or streaming services 
                rely more on raw throughput. The data traffic with databases is generally rather small, therefore
                IOPS\footnote{Input/Output Operations per Seconds} are more important.

            \paragraph{Administration}

                One important reason to do reasearch in this topic is to evaluate if the time and effort to
                set up a cluster brings benefits in terms of administrative expenditure. At first sight a viable
                metric could be the spent time from starting to have a cluster up and running. However, depending
                on the production environment the results may vary to a wide degree. Therefore chapter \ref*{analysis:administration}
                will try to generate more abstract metrics for an objective evaluation.


        \subsection{Related Work}
        
            Since containerization and CEPH are already well established technologies, this section gives a brief overview of 
            other research which was already conducted in this area.
            RedHat in cooperation with Percona and Supermicro already conducted extensive research wheter it is feasable to
            run a SQL-Database on a CEPH-Cluster\cite{redhatstudy}. They concluded that indeed running a database on an 
            optimized CEPH-cluster exceeds industry standard database solutions. Furthermore scaling horizontally 
            is easier with a CEPH Cluster. Nevertheless it should be mentioned, that in this study the Percona 
            SQL distribution was utilized, which provides a native interface for kernel based RBD\footnote{Replicated Block Devices}.
            
            Hong et.al. used CEPH to create a framework which aims to mitigate handling issues with database containers.
            This includes the issues mentioned in chapter\ref*{introduction} that containers aren't persistent ways
            to store data\cite{hong2019database}. However this paper does not use a CEPH-cluster to host a database and
            has therefore only very little relevance to the topic of this paper. 
            
            Although there are numerous other papers
            which discuss the process of setting up CEPH itself or in the context of an OpenStack environment,
            none of them discussed a similar scope.            

    \section{Setting up CEPH on Docker}

        This section is about setting up CEPH with containerization. Firstly, an extensive insight into CEPH and its underlying
        structure is provided. Secondly The system-architecture for the experiments in the scope of this paper is explained.
        This second section will make a small detour into containerization, what it means, what the benefits and the drawbacks are.

        \subsection{CEPH Architecture}\label{ceph:arch}

            In short CEPH is a distributed solution for storage clusters. On one side it is specifically tailored to provide
            maximum reliability by distributing data over different disks, machines or even datacenters and therefore also 
            improving overall scalability and performance. On the other side it is also a cost effective because it is open source\textbf{[Github Link]}
            and runs on commodity hardware. 

            The general structure includes several services which manage different functions within the cluster. In the context of
            CEPH these are called deamons, which are discussed in detail in the following sections.

            \begin{figure}[H]
                \centering
                \fbox{\includegraphics[width=\imgScale\textwidth]{ceph-architecture.jpg}}
                \caption{CEPH Architecture, \cite{hadlich_2015}}
                \label{fig:ceph-arch}
            \end{figure}
            
            Because it is a distributed system and may span over several physical machines, racks or even datacenters, inter-daemon
            communication plays a key role. For the purpose of autonomous data replication the cluster relies on a dedicated network.
            This network is separated from the public network, which is for client data access. Figure \ref{fig:ceph-arch} provides a 
            general overview.

            How the data is distributed internally is determined by two main factors: \textit{Placement Groups} and the \textit{CRUSH} algorithm.
            Placement Groups (abbreviated \textit{PGs}) are a way to keep track of the physical location of objects.
            Files are split up into different objects, where each object is stored in a different PG. Which one is determined by the 
            CRUSH\footnote{Controlled, Scalable, Decentralized Placement of Replicated Data} algorithm\cite{weil2007ceph}. Figure \ref{fig:ceph-crush}
            shows a schema of how this works.
            The reasoning behind PGs is that keeping track of millions of objects via metadata is computationally expensive\cite{PGcephDocu}.
            Therefore Objects are hashed and assigned to PGs.
            A so called CRUSH-Map keeps track of the structure of the whole cluster. This will become important in section \ref{system:crush-fail}.

            \subsubsection{Cluster Access}

                To access the cluster ceph provides different interfaces depending on the use-case:

                \begin{itemize}
                    \item \textit{CephFS} - A POSIX\footnote{Portable Operating System Interface - Part of the Unix specification} conform filesystem
                    \item \textit{LIBRBD/KRBD} - A block device
                    \item \textit{RADOSGW} - An REST gateway for storage buckets
                    \item \textit{LIBRADOS} - The API to access the cluster directly from applications
                \end{itemize}

                All these interfaces are based on LIBRADOS, provides the interface to access RADOS. This is the underlying object store
                responsible for distributing the data over several phsyical disks. This is done by the \textit{OSD}-Daemon.

                \paragraph{CephFS} is the filesystem, which allows for CEPH to behave to the user or the operating system like any other
                filesystem would. However is has to be distinguished from from block device. 
                The key is that it is not a filesystem like EXT4 or NTFS. It is more a translation layer to the
                RADOS\footnote{Reliable Autonomous Distributed Object Storage} interface, and provides features like snapshots. A required metadata service called \textit{MDS} provides journaling functionality and handle
                multi user data access. Because the throughput of the whole cluster scales linearly with the size of the cluster,
                often several different MDSs are required to handle the load and distribute the file metadata mitigate
                single points of failure. Is is best practice store and server the MDS from fast solid state drives to migigate bottlenecks.

                \paragraph{LIBRBD/KRBD} is a virtual block device. The difference to CephFS is that is behaves like a physical disk.
                Therefore it can be formatted with any filesystem like EXT4 or NTFS. The distinction between \textit{LIB}RBD and \textit{K}RBD
                means userspace for LIBRBD and kernelspace for KRBD. To explain these differences between these two would go beyond the 
                scope of this paper and requires knowledge about kernels, operating systems and memory handling. 

                \paragraph{RADOSGW} provides as REST gateway access storage buckets. Unlike filesystems or block devices, these storage
                buckets don't have a hierarchical structure with directories. All objects are stored on the same level, therefore the 
                analogy to a bucket. This bucket is addressed by means of an API which provides also a metadata filter to select
                specific files directly. This way of storing data is also compatible to Amazons S3\footnote{Simple Storage Service} product.
                A widely adopted usecase is serving static files on the internet for media content, javascript or css files by content delivery networks.
                They can also be used to store virtual disk images to be used in conjunction with RBDs.

                \paragraph{LIBRADOS} provides a native API to access RADOS directly without any file system or block device translation layer in
                between. The higher development costs for a more complex implementation might be worth the gains in throughput and IOPS
                for critical applications.

            \subsubsection{Object Storage Devices - OSD}

                Within the CEPH domain, these are the disks, where the actual data objects are stored to. 
                One or more OSDs can be handled from one CEPH OSD Daemon, which provides the connecting layer to the whole cluster.
                Also one OSD can contain several Placement Groups. This depends on the configuration of the cluster. On which disk and in which PG
                and object is stored is determined by the CRUSH algorithm as shown in figure \ref{fig:ceph-crush}.
                Since data is replicated within the cluster and constantly updated, one OSD could fail and the data and PGs of this failed
                OSD will be written to another OSD. This constant data shuffling takes place transparent to the user and is the reason why CEPH
                needs a dedicated network as mentioned in section \ref{ceph:arch}. How the data is physically stored on the disk differs between
                the two different backends used in older or newer releases.
            
            \subsubsection{Monitor Nodes - MON}

                Monitor Nodes (abbreviated as \textit{MON}) have two main purposes. They maintain a copy of the cluster map to hand this out 
                to a client, which connects to the cluster. Because of a fault this map might not reflect the state of the cluster accurately.
                Therefore an odd number of MON nodes have to agree on the current state of the cluster and distribute the correct cluster map.
                According to the CEPH documentation this is negotiated via the \textit{PAXOS} algorithm\cite{MONcephDocu}.
                Although a cluster could technically work with just one MON node, it is highly recommended to have at least three MON nodes,
                preventing a single point of failure.

                \begin{figure}[H]
                    \centering
                    \fbox{\includegraphics[width=\imgScale\textwidth]{Ceph-crush-map.png}}
                    \caption{Placement Groups and CRUSH Algorithm, \cite{krenn2020}}
                    \label{fig:ceph-crush}
                \end{figure}                           

            \subsubsection{Metadata Server - MDS}

                The metadata server is not neccessary for the cluster to operate as RBD, storage bucket or via Librados API.
                However to provide a journaled filesystem via the CephFS, this service is needed to provide the filesystem metadata such
                as permissions and timestamps. As stated from RedHat, it is best practice to deploy a MDS with big and fast nVME drives\cite{redhatstudy}.
                These can be also used in conjunction with memcached, where the recent metadata are stored into the RAM of the MDS.
                Overprovisioning, automatic failover and scalability are the main considerations for MDS to have high availability and high performance.

            \subsubsection{Manager - MGR}   
            
                Since the 12.x release, CEPH needs a manager node to operate\cite{MGRcephDocu}. This manager does not provide any
                functionality to the cluster itself but acts as an interface to external monitoring tools and systems. Without it 
                the state of the cluster won't appear to be healthy nor be externally visibly updated\cite{MGRcephDocu}.                

        \subsection{System Architecture}\label{system:my-setup}

        % three servers, one mon per machine
        % different number of OSD per machine
        % connected via docker network bridge

            \subsubsection{Containerization}\label{system:containerization}

                %whats a container and why is it nice compared to VM, etc.
                Although in the previous text the term containerization was mentioned a few times already, there was no
                definitive definition of what a container or more general containerization. Firstly it is important to establish
                that containers are not \textit{something like} a virtual machine. In fact virtual machines are entire systems 
                which are executed on the hosts hypervisor\footnote{An abstraction layer for hardware and operating systems} and entirely self contained.
                This approach is really useful to deploy and maintain infrastructure since a machine with a defined behaviour can be
                launched in little time with no effort. However in terms of application development or microservices especially in the realm
                of CI/CD\footnote{Continuous Integration/Continuous Deployment} this is an uneccessary overhead.
                Containers on the other hand are in comparison small entities, which contain only the neccessary components
                of an application or service to run.

                \begin{figure}[H]
                    \centering
                    \fbox{\includegraphics[width=\imgScale\textwidth]{docker-vs-vm.png}}
                    \caption{Containerization compared to Virtualization, \cite{2021_docker}}
                    \label{fig:docker-vs-vm}
                \end{figure}     

                By deploying applications or services this way, problems such as confliciting software versions or 
                applications can be encapsulated, hence solved. This makes scaling also easier, since scaling doesn't
                have to happen on a virtual machine level, which might cause additional costs for licensing, 
                services can be scaled on a more granular basis with containers and orchestration tools like
                \textit{Docker Swarm} or \textit{Kubernetes}, which will be explained in detail in section \ref{system:containerization}.
                As of early 2021, there exists a vast variety of different containerization solutions such as 
                Rkt, LXD and Docker\cite{cncf}. However Docker is unarguably the biggest solution in terms of 
                adoption, available images\footnote{Docker images are the blueprints to start containers from} and community.

            \subsubsection{Orchestration with Kubernetes (or Docker Swarm maybe...)}\label{system:orchestration}

            %etcd (KV storage), pods, ceph, mysql

            \subsubsection{Docker Config for CEPH Image}

            %OSD
            % container config: different config options:
            % essentially: how much work is abstracted away from container             

            \subsubsection{CRUSH Fail mode}\label{system:crush-fail}

                In order to define the behaviour of the cluster in case of failures, CEPH provides a setting
                which is called \textit{CRUSH Fail Mode}. This setting defines the biggest possible failure domains, 
                which should not affect data integrity\cite{crushFailure}. In consequence, data is replicated in a
                way which guarantees this. To pick a few examples: 

                \paragraph{OSD (or device)} means that a single drive could fail and no data will be lost.
                This would be the preferred setting when the cluster runs on a single machine. However, if the cluster
                consists of more than one machine and the whole machine fails because of a PSU\footnote{Power Supplying Unit} failure
                for example, data integrity cannot be guaranteed anymore.

                \paragraph{Chassis} is the failure mode for sophisticated rack mounted servers, where a multitude of servers
                comprise the whole cluster. This would cover the previously mentioned case, which might be as well interesting for maintenance,
                when individual machines have to be maintained or decommissioned.
                
                \paragraph{Rack} provides data integrity across multiple chassis or hosts on a datacenter level. If for example
                a critical component of the network infrastructure fails, this could provide some security. Although CEPH clusters 
                are in principle set up in a way which prevents SPOF\footnote{Single Point Of Failure}, this must not be true for the
                peripheral infrastructure. 

                \paragraph{Datacenter} usually means that a huge number of several racks, consisting of multiple chassis are grouped
                together as a common failure domain - in case of a power outage or natural disaster. If the company which owns the datacenter,
                operates on a global scale this failure mode is also a consideration for performance. Because of physical and network
                topology, the RTT\footnote{Round Trip Time} to a certain datacenter might be longer than desired. If all data is replicated
                on datacenter granularity, a global scale data integrity is guaranteed as well as the possibly shortest response time.

                \paragraph{In general} the failure mode should be as granular as possible but also as fine as financially feasable.
                In the context of the system architecture in the scope of this paper (see section \ref{system:my-setup}), the fail mode will be on an OSD level.
                Because my experimental setup consists of multiple disks and flash storages with different ages and levels of degradation.
                However, the appropriate setting would be \textit{host}-level data replication. \textbf{[Revisit this section after finished setup]}

            \subsubsection{Issue with Docker Image}

            % refer to https://hub.docker.com/r/ceph/daemon 
            % workaroung with either multiple OSD per container (undesireable)
            % or different thing -> What are the implications anyway?

        
    \section{Database considerations}

        \subsection{Databases}

        % quick rundown. Focussed on mySQl, mention of noSQL, etc.

        \subsection{Architecture of mySQL}

            % why onfirst sight it might not be ideal to deploy mySQL on ceph because of distributed nature
            % and in case of Table > volume, etc.
            % architecture: https://www.youtube.com/watch?v=CCqFqraSQQ0

            % whats acid and why is it important in this context
            % is it actually an issue?!            

            \subsection{ACID}

            % Atomic, Consistent, Isolated, Durable
            % https://www.youtube.com/watch?v=VRm2UMsFVz0

            \paragraph{Atomic}

            \paragraph{Consistent}

            \paragraph{Isolated}

            \paragraph{Durable}


            \paragraph{Example: Bank Account}

            % i.e. concurrent transactions
            % example: bank account

        \subsection{Problems with clusters}\label{database:cluster}

            

        %sharding and distribution techniques
        %splitting up of big files over OSD

        \subsection{Considerations for this research}

    \section{System Analysis}

        % what are the key points to analyze in the context

        \subsection{Disclaimer}

        [Bc of Corona I can't use lab therefore only my setup. Discuss some shortcomings and implications.]

        % my setup is a multi tennant system, so take everything with a grain of salt.
        % however: disks were single tennant

        %cannot setup "native" environment, make reasonable assumptions based on other research

        \subsection{Data Integrity}

        %distribution over PG, OSD, Hosts
        %bluestore vs posix

        %CEPH data striping:
        % https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/1.2.3/html/red_hat_ceph_architecture/ceph_client_architecture#:~:text=Object%20Size%3A%20Objects%20in%20the,%2C%204MB%2C%20etc.).

        \subsection{Performance Penalty}\label{analysis:penalty}

        %CephFS vs. Librados vs. native
        %Bluestore vs POSIX

        % IOPS vs raw throughput
        % the point is: does docker make a difference here?

        \subsection{Administration}\label{analysis:administration}

        % Ceph Dashboard
        % Grafana mySQL        
        
        %first thought: no benefit in using containers to save time on setting up
        %rather tediously trying to figure out how to set everything up - documentation rather sparse (generic examples)
        %error with config -> fixed by using nautilus release


        \subsection{Tuning}\label{analysis:tuning}

        [very briefly]

        % KVM vs QEMU
        % librbd vs krbd
        % percona container in krbd module vs. mysql docker on librbd
        % other tuning parametres

    \section{Conclusion}

        \subsection{Advantages}

        %in comparison to conventional (Host Level)

        \subsection{Disadvantages}

        %in comparison to conventional (Host Level)        

        \subsection{Performance}
    
        % make remark about performance tuning and tiering -> beyond the scope of this
        % i.e. pool configs, stripe config, safety with crush config, etc.
        % also there might be another interface for mySQL needs like RADOSGW or RBD
        % but this would not be comparable to a single tennant mySQL application

        %which implementation of radoes does the container use? krbd vs librbd
        % remark to section with OSD in Architecture: "ease of use" vs "performance" tradeoff

        \subsection{In Summary}

        % summarize the keypoints in regard to the introduction
      

    \bibliography{references}        
    \bibliographystyle{IEEEtran}

\end{document}