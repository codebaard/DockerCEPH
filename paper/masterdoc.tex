\documentclass[titlepage, a4paper, 11pt]{scrartcl}

%too much whitespace otherwise
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}

% Grafik Pakete
\usepackage{graphicx,hyperref,amssymb}

% Ordner fÃ¼r Grafiken
\graphicspath{ {./images/} }
\usepackage{float}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}

\usepackage{caption}
\usepackage{subcaption}

% Header and Footer
\usepackage{fancyhdr}

%bibtex
\usepackage{cite}

%code snippets
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
    language=HTML,
    aboveskip=3mm,
    belowskip=3mm,
    showstringspaces=false,
    columns=flexible,
    basicstyle={\small\ttfamily},
    numbers=none,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue},
    commentstyle=\color{dkgreen},
    stringstyle=\color{mauve},
    breaklines=true,
    breakatwhitespace=true,
    tabsize=3
}

\pagestyle{fancy}
\fancyhf{}
\rhead{Julius Neudecker, 2025850}
\lhead{CEPH Cluster in containers}

\title{Running a CEPH-Cluster on a containerized infrastructure}
\subtitle{Use case: distributed mySQL-database}
\author{Julius Neudecker \\ Bachelor of Science \\ \href{mailto:julius.neudecker@haw-hamburg.de}{julius.neudecker@haw-hamburg.de}}
\date{January 2020}


\begin{document}

    \maketitle

    \tableofcontents

    \begin{abstract}
        Setting up and operate a storage cluster with high availability is a complex task. Modern paradigmas like containerization
        and orchestration are a way of abstracting away some complexity. However, running a cluster in a stateless and ephemeral
        containerized environment poses some problems. In the following paper these problems are identified and scrutinized.
        The use case will be a mySQL database, which will be stored on a CEPH cluster comprised of docker based daemons.
    \end{abstract}

    \section{Introduction}
        %Managing a highly available storage cluster is not a trivial task. Apart from provisioning and monitoring the hardware,
        %setting up multiple systems concurrently is a daunting task. Nowadays with provisioning tools like Salt, Chef or Puppet, 
        %this is easier than ever. However

        %Docker, stateless, ephemeral vs database, acid,

        %do some sidestep to open stack and the RedHat Paper and why ceph blabla

        %do some explanation about single/multi tennant and cloud

        %put the whole paper in the context of IAAS; PAAS; SAAS

        % make a remark on which particular issue this paper adresses, since there are way too many 
        % to discuss and thats beyond this paper.

        \subsection{CEPH Based storage cluster}

        % librados, radosgw, rbd, cephfs (build on top of krbd/librados -> source?)
        % quick rundown of MON, OSD, MDS, MGR

        \subsection{Containerization}

        %whats a container and why is it nice compared to VM, etc.

        \subsection{Deploying}        

        % with/without container

        \subsection{Databases}

        % quick rundown. Focussed on mySQl, mention of noSQL, etc.

        \subsection{Scope of the problem and definition of research goal}

        %related work: redhat document

    \section{Setting up CEPH on Docker}

        \subsection{System Architecture}

        % three servers, one mon per machine
        % different number of OSD per machine
        % connected via docker network bridge

            \subsubsection{CRUSH Fail mode}

            % elaborate different settings: Datacenter, Room, Row, Rack, Host, OSD...
            % this has to be taken into consideration for providing the data on a global scale

            % host vs osd in this case

            % assuming all disks are _equally_ new, host mode would be appropriate in this setting
            % in this particular case I would choose OSD level since disks are of different type, age and degradation

            \subsubsection{Issue with Docker Image}

            % refer to https://hub.docker.com/r/ceph/daemon 
            % workaroung with either multiple OSD per container (undesireable)
            % or different thing -> What are the implications anyway?

        \subsection{Monitor Nodes}

        % even/odd number and therefore need for odd number of servers

        \subsection{Object Storage Devices - OSD}

        % container config: different config options:
        % essentially: how much work is abstracted away from container

        \subsection{Metadata Service}

        \subsection{Manager}
        
    \section{Database considerations}

        \subsection{Architecture of mySQL}

        % why onfirst sight it might not be ideal to deploy mySQL on ceph because of distributed nature
        % and in case of Table > volume, etc.
        % architecture: https://www.youtube.com/watch?v=CCqFqraSQQ0

        \subsection{ACID}

        % Atomic, Consistent, Isolated, Durable
        % https://www.youtube.com/watch?v=VRm2UMsFVz0

        % whats acid and why is it important in this context
        % is it actually an issue?!

        % i.e. concurrent transactions
        % example: bank account

        \subsection{Problems with clusters}

        %sharding and distribution techniques
        %splitting up of big files over OSD

        \subsection{Scope of this paper}

    \section{System Analysis}

        % what are the key points to analyze in the context

        \subsection{disclamer bc of my setup}

        % my setup is a multi tennant system, so take everything with a grain of salt.
        % however: disks were single tennant

        %cannot setup "native" environment, make reasonable assumptions based on other research

        \subsection{Integrity}

        %distribution over PG, OSD, Hosts

        %CEPH data striping:
        % https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/1.2.3/html/red_hat_ceph_architecture/ceph_client_architecture#:~:text=Object%20Size%3A%20Objects%20in%20the,%2C%204MB%2C%20etc.).

        \subsection{Penalty}

        %CephFS vs. Librados vs. native

        % IOPS vs raw throughput
        % the point is: does docker make a difference here?

        \subsection{Tuning}

        % KVM vs QEMU
        % librbd vs krbd
        % percona container in krbd module vs. mysql docker on librbd
        % other tuning parametres

        \subsection{Administration}

        % Ceph Dashboard
        % Grafana mySQL          

    \section{Conclusion}

        \subsection{Advantages}

        %in comparison to conventional (Host Level)

        \subsection{Disadvantages}

        %in comparison to conventional (Host Level)        

        \subsection{Performance}
    
        % make remark about performance tuning and tiering -> beyond the scope of this
        % i.e. pool configs, stripe config, safety with crush config, etc.
        % also there might be another interface for mySQL needs like RADOSGW or RBD
        % but this would not be comparable to a single tennant mySQL application

        %which implementation of radoes does the container use? krbd vs librbd
        % remark to section with OSD in Architecture: "ease of use" vs "performance" tradeoff

        \subsection{In Summary}

        % summarize the keypoints in regard to the introduction
      

    \bibliography{literature}        
    \bibliographystyle{IEEEtran}

\end{document}